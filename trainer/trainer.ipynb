{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: transformers in /home/sadeghi/.venv/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Looking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: datasets in /home/sadeghi/.venv/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: huggingface_hub in /home/sadeghi/.venv/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: datasets in /home/sadeghi/.venv/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: python-dotenv in /home/sadeghi/.venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub datasets python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file is saved at: ../data/models--mhmsadegh--fashion_jewelry_cosmetics_persian_names/snapshots/1d0ee1d0834da7260405d98b60fd5850a97e3655/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Define the model or dataset repo\n",
    "# repo_id = \"mhmsadegh/fashion_jewelry_cosmetics_persian_names\"\n",
    "# filename = \"dataset.csv\"\n",
    "\n",
    "# # Specify the target directory where you want to save the file\n",
    "# target_directory = \"../data/\"\n",
    "\n",
    "# # Ensure that the target directory exists\n",
    "# os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# # Download the file and specify the cache directory\n",
    "# file_path = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=target_directory)\n",
    "\n",
    "# # Print the location where the file has been saved\n",
    "# print(f\"Downloaded file is saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Dataset: Dataset({\n",
      "    features: ['Product_name', 'label'],\n",
      "    num_rows: 1092\n",
      "})\n",
      "Cleaned Validation Dataset: Dataset({\n",
      "    features: ['Product_name', 'label'],\n",
      "    num_rows: 273\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "train_address = \"../data/models--mhmsadegh--fashion_jewelry_cosmetics_persian_names/snapshots/1d0ee1d0834da7260405d98b60fd5850a97e3655/dataset.csv\"\n",
    "dataset = load_dataset('csv', data_files={'all_data': train_address})\n",
    "\n",
    "# Maps for your labels\n",
    "label_map = {'Cosmetics': 0, 'Jewerly': 1, 'Fashion': 2, 'Others': 3}\n",
    "\n",
    "# Convert labels to integers\n",
    "dataset = dataset.map(lambda x: {'label': label_map[x['Lable']]}, remove_columns=[\"Lable\"])\n",
    "\n",
    "# Split into train and validation sets\n",
    "def split_train_val(dataset, test_size=0.2, seed=42):\n",
    "    # Extract the dataset into pandas DataFrame\n",
    "    df = dataset['all_data'].to_pandas()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df[\"label\"])\n",
    "\n",
    "    # Convert the splits back to Hugging Face datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = split_train_val(dataset)\n",
    "# Remove the '__index_level_0__' column\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
    "val_dataset = val_dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "# Verify the cleanup\n",
    "print(\"Cleaned Train Dataset:\", train_dataset)\n",
    "print(\"Cleaned Validation Dataset:\", val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "c_model = \"sentence-transformers/LaBSE\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(c_model ) \n",
    "\n",
    "# Step 2: Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(c_model, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4503795e3794033ac16f62ee3781a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698be50def194cd69e1e8fd60fc570ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenization and padding (you can adjust max_length)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['Product_name'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadeghi/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",           # save model every epoch\n",
    "    load_best_model_at_end=True,     # load best model when finished training (best on validation)\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy to select the best model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 5: Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=1)  # Get the predicted class index\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # evaluation metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='411' max='411' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [411/411 01:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.354637</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.384319</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.391524</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=411, training_loss=0.2774826746474761, metrics={'train_runtime': 101.9596, 'train_samples_per_second': 32.13, 'train_steps_per_second': 4.031, 'total_flos': 215491823898624.0, 'train_loss': 0.2774826746474761, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for input 'شلوار پسرونه' is: Fashion\n"
     ]
    }
   ],
   "source": [
    "def predict_label(model, tokenizer, text, label_map, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predicts the label for a given text input using a fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The fine-tuned Hugging Face model.\n",
    "    - tokenizer: The tokenizer used for tokenizing the input.\n",
    "    - text: The input text to classify.\n",
    "    - label_map: A dictionary mapping labels to their corresponding indices.\n",
    "    - device: The device to run the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - The predicted label as a string.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess the input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    # Move inputs to the device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Find the label name from the map\n",
    "    inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    predicted_label = inverse_label_map[predicted_label_idx]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_input = \"شلوار پسرونه\"  # Replace with your test string\n",
    "predicted_label = predict_label(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text_input,\n",
    "    label_map=label_map,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"Predicted label for input '{text_input}' is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9574132350844b6a619d0a95c214ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baa045dfda64ddc8ebd463b51c634fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901a05383d4e44619b3c6e0b2ee21de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully pushed to: https://huggingface.co/mhmsadegh/fine_tuned_bert_multiclass_FashionCosmetics_Jewerly\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Step 8: Save the model and tokenizer locally\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Step 9: Push the model and tokenizer to Hugging Face Hub under your account\n",
    "model_name = \"fine_tuned_bert_multiclass_FashionCosmetics_Jewerly\"  # Name for your model repository\n",
    "repo_name = f\"mhmsadegh/{model_name}\"     # Full repository name under your account\n",
    "\n",
    "# Push the model\n",
    "model.push_to_hub(repo_name, commit_message=\"Add fine-tuned BERT model for multiclass classification\")\n",
    "\n",
    "# Push the tokenizer\n",
    "tokenizer.push_to_hub(repo_name, commit_message=\"Add tokenizer for fine-tuned BERT model\")\n",
    "\n",
    "print(f\"Model successfully pushed to: https://huggingface.co/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
