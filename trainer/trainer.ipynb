{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: transformers in /home/sadeghi/.venv/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sadeghi/.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Looking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: datasets in /home/sadeghi/.venv/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi:****@nexus.modaai.lol/repository/pypi-proxy/simple/\n",
      "Requirement already satisfied: huggingface_hub in /home/sadeghi/.venv/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: datasets in /home/sadeghi/.venv/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: python-dotenv in /home/sadeghi/.venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sadeghi/.venv/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/sadeghi/.venv/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sadeghi/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sadeghi/.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sadeghi/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sadeghi/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub datasets python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `mistral` has been saved to /home/sadeghi/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/sadeghi/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file is saved at: ../data/models--mhmsadegh--fashion_jewelry_cosmetics_persian_names/snapshots/1d0ee1d0834da7260405d98b60fd5850a97e3655/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model or dataset repo\n",
    "repo_id = \"mhmsadegh/fashion_jewelry_cosmetics_persian_names\"\n",
    "filename = \"dataset.csv\"\n",
    "\n",
    "# Specify the target directory where you want to save the file\n",
    "target_directory = \"../data/\"\n",
    "\n",
    "# Ensure that the target directory exists\n",
    "os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "# Download the file and specify the cache directory\n",
    "file_path = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=target_directory)\n",
    "\n",
    "# Print the location where the file has been saved\n",
    "print(f\"Downloaded file is saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Train Dataset: Dataset({\n",
      "    features: ['Product_name', 'label'],\n",
      "    num_rows: 1092\n",
      "})\n",
      "Cleaned Validation Dataset: Dataset({\n",
      "    features: ['Product_name', 'label'],\n",
      "    num_rows: 273\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "train_address = \"../data/models--mhmsadegh--fashion_jewelry_cosmetics_persian_names/snapshots/1d0ee1d0834da7260405d98b60fd5850a97e3655/dataset.csv\"\n",
    "dataset = load_dataset('csv', data_files={'all_data': train_address})\n",
    "\n",
    "# Maps for your labels\n",
    "label_map = {'Cosmetics': 0, 'Jewerly': 1, 'Fashion': 2, 'Others': 3}\n",
    "\n",
    "# Convert labels to integers\n",
    "dataset = dataset.map(lambda x: {'label': label_map[x['Lable']]}, remove_columns=[\"Lable\"])\n",
    "\n",
    "# Split into train and validation sets\n",
    "def split_train_val(dataset, test_size=0.2, seed=42):\n",
    "    # Extract the dataset into pandas DataFrame\n",
    "    df = dataset['all_data'].to_pandas()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=seed, stratify=df[\"label\"])\n",
    "\n",
    "    # Convert the splits back to Hugging Face datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = split_train_val(dataset)\n",
    "# Remove the '__index_level_0__' column\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
    "val_dataset = val_dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "# Verify the cleanup\n",
    "print(\"Cleaned Train Dataset:\", train_dataset)\n",
    "print(\"Cleaned Validation Dataset:\", val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "c_model = \"sentence-transformers/LaBSE\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(c_model ) \n",
    "\n",
    "# Step 2: Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(c_model, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9192e44ada2a4bfbbb6158d2f94f7cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baa1e25569a45c485c44816e4eec9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/273 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenization and padding (you can adjust max_length)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['Product_name'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadeghi/.venv/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",           # save model every epoch\n",
    "    load_best_model_at_end=True,     # load best model when finished training (best on validation)\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy to select the best model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 5: Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=1)  # Get the predicted class index\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # evaluation metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 11.73 GiB of which 70.69 MiB is free. Including non-PyTorch memory, this process has 11.65 GiB memory in use. Of the allocated memory 10.57 GiB is allocated by PyTorch, and 914.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/sadeghi/Image_prj/Bertmulticlass_classification/trainer/trainer.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d595553455234303730227d/home/sadeghi/Image_prj/Bertmulticlass_classification/trainer/trainer.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Step 7: Fine-tune the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d595553455234303730227d/home/sadeghi/Image_prj/Bertmulticlass_classification/trainer/trainer.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/trainer.py:2577\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         grad_norm \u001b[39m=\u001b[39m _grad_norm\n\u001b[1;32m   2575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_pre_optimizer_step(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2577\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   2579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_optimizer_step(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   2581\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/accelerate/optimizer.py:178\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_state\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mXLA:\n\u001b[1;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_state\u001b[39m.\u001b[39mis_xla_gradients_synced \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[39m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[39m.\u001b[39m_opt_called \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49m\u001b[39m__get__\u001b[39;49m(opt, opt\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:209\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    206\u001b[0m     amsgrad: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m cast(Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m], group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 209\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m    212\u001b[0m         grads,\n\u001b[1;32m    213\u001b[0m         amsgrad,\n\u001b[1;32m    214\u001b[0m         exp_avgs,\n\u001b[1;32m    215\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    216\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    220\u001b[0m     adamw(\n\u001b[1;32m    221\u001b[0m         params_with_grad,\n\u001b[1;32m    222\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         has_complex\u001b[39m=\u001b[39mhas_complex,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:148\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    138\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m     torch\u001b[39m.\u001b[39mzeros(\n\u001b[1;32m    140\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, dtype\u001b[39m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(\n\u001b[1;32m    149\u001b[0m     p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    152\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(\n\u001b[1;32m    153\u001b[0m     p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format\n\u001b[1;32m    154\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 11.73 GiB of which 70.69 MiB is free. Including non-PyTorch memory, this process has 11.65 GiB memory in use. Of the allocated memory 10.57 GiB is allocated by PyTorch, and 914.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Step 7: Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set optimizer and loss function for model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadeghi/.venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_lables = len(set(train_df['Lable']))\n",
    "epoch = 4\n",
    "batch_size = 32 \n",
    "lr = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BaseModel()\n",
    "optimizer = AdamW(model.parameters(), lr=lr) \n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8ee4bc994d40e49b9a5a03dd8e114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,training loss: 1.31 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90babb86d534e74ab2dc07a2807a6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,training loss: 1.32 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe91bded0f4440988dfaba328667bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2,training loss: 1.30 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42011de8796e4e698ac2863587f74159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3,training loss: 1.29 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4431b3753b49a6ab22d0a2c177fe50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4,training loss: 1.28 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f55bb63c1b4ebd9565223d21cc5c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5,training loss: 1.28 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7888f48e75494ae9804e544e3e30219a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6,training loss: 1.27 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6ff8bcb50d4ec185d07143ca05bbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7,training loss: 1.27 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2428cdefce748b9a732ab9f1022388d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8,training loss: 1.27 , train accuracy: 0.55 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e06ce0e5f64d5d8ae5fca3b96927f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9,training loss: 1.27 , train accuracy: 0.55 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset , batch_size=batch_size)\n",
    "train(model , optimizer , loss_fn , train_loader , 10 , device )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label is: Others\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text_input, device=\"cpu\"):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Perform the prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Check if the output contains logits or is just a tensor\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits = outputs[0]  # The first element in the tuple is the logits\n",
    "        else:\n",
    "            logits = outputs  # If it's not a tuple, directly use the output as logits\n",
    "\n",
    "    # Convert logits to predicted class (max probability)\n",
    "    prediction = torch.argmax(logits, dim=1).cpu().numpy().item()\n",
    "\n",
    "    # Map predicted index to label\n",
    "    maps = {'Cosmetics': 0, 'Jewerly': 1, 'Fashion': 2, 'Others': 3}\n",
    "    reverse_maps = {v: k for k, v in maps.items()}\n",
    "    predicted_label = reverse_maps[prediction]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example Usage:\n",
    "text_input = \"رژ لب\"\n",
    "predicted_label = predict(model, text_input, device=\"cuda\")  # Use \"cuda\" if you're using a GPU\n",
    "print(f\"The predicted label is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model in Huggingface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from huggingface_hub import login, upload_file, create_repo\n",
    "import os\n",
    "\n",
    "# Authenticate (if not already done)\n",
    "# login()  # Uncomment this if not logged in\n",
    "\n",
    "# Define your model, tokenizer, and repo\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)  # example, replace with your trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # example, replace with your tokenizer\n",
    "\n",
    "# Path to save model\n",
    "model_save_path = \"./model\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Create a new model repo on Hugging Face (if not already created)\n",
    "repo_name = \"your-username/your-model-repo\"  # Replace with your model repo name\n",
    "create_repo(repo_name, exist_ok=True)  # This will create the repo if it doesn't already exist\n",
    "\n",
    "# Push model files to Hugging Face\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# Upload the model folder to Hugging Face\n",
    "upload_folder(\n",
    "    repo_id=repo_name,\n",
    "    folder_path=model_save_path,\n",
    "    path_in_repo=\"\"  # You can specify a subdirectory within your repo if desired\n",
    ")\n",
    "\n",
    "print(f\"Model pushed to Hugging Face repo {repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
